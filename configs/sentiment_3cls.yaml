# Minimal config for SST-2 (binary) sentiment fine-tuning
project: post_train_learn
seed: 42

model:
  name_or_path: Qwen/Qwen2-1.5B-Instruct
  max_length: 256
  use_chat_template: false

qlora:
  load_in_4bit: true
  bnb_4bit_compute_dtype: bfloat16
  bnb_4bit_quant_type: nf4
  bnb_4bit_use_double_quant: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.05
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

data:
  train_file: data/processed/train.jsonl
  dev_file: data/processed/dev.jsonl
  test_file: data/processed/test.jsonl
  text_field: text
  label_field: label
  labels: [negative, positive]

training:
  output_dir: outputs
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  learning_rate: 2e-5
  num_train_epochs: 3
  gradient_accumulation_steps: 4
  eval_steps: 200
  save_steps: 200
  logging_steps: 50
  fp16: false
  bf16: true
